TODO:

- Multi-Label Training und Evaluation umsetzen

- Verschiendene Losses Testen, siehe
    - https://towardsdatascience.com/handling-class-imbalanced-data-using-a-loss-specifically-made-for-it-6e58fd65ffab
    - https://pytorch.org/docs/stable/generated/torch.nn.MultiLabelSoftMarginLoss.html#torch.nn.MultiLabelSoftMarginLoss
    - Hamming loss (minimiert die Hamming Distance): https://scikit-learn.org/stable/modules/model_evaluation.html#hamming-loss
    - Class Balanced Loss: https://github.com/vandit15/Class-balanced-loss-pytorch/blob/master/class_balanced_loss.py
    - Softmax for Multilabel Classification (Faceboook): https://gombru.github.io/2018/05/23/cross_entropy_loss/

- Hyperparameter-Tuning: Frameworks/Libs finden und für eine entscheiden
    - Ray Tune: https://docs.ray.io/en/master/tune/index.html
    - Optuna: https://github.com/optuna/optuna
    - Hyperopt: https://github.com/hyperopt/hyperopt

- Mehr Metriken wegspeichern, nicht nur Loss und die Accuracy:
    * So viele wie möglich  -> Acc, Precision, Recall, F1, AUC/ROC, 4 Felder für jede Klasse (FP, FN, TP, TN)
                            -> Weitere, eigene Ideen: PR-Curve
    * Beim Testen für den jeweilgen Datensatz, welche WK er für welche Klasse vorhersagt (nicht beim Validieren, einmal am Ende)

- Mit Tensorboard und den zur Verfügung stehenden Visualisierung befassen; Zusammenspiel mit Framework verstehen

- Mapping von Orderstruktur in /saved auf mehr Informationen, damit es auch in Monaten noch nachvollziehbar ist
    * Neues Dokument im Cherrytree anlegen und dort das Mapping erstellen
    * Dazu schreiben, was man da speziell gemacht hat -> Codierung überlegen, wie man einzelne Läufe anhand des Namens erkennt
    * Beispielsweise enthält die Codierung von Robert folgende Elemente (teils als Zahlencode):
      Modellname, Pretrain oder FineTune, Datensatz, Modelltyp, feine Aenderung

- Daten nach der Vorverarbeitung plotten und anschauen
    * Passt das so, wie es aus dem Dataset kommt?
    * Ist bei der Vorverarbeitung was schief gegangen? Fehlen Werte?

- In der _save_checkpoint Methode des BaseTrainers  wird nur der Klassenname gespeichert, der Code hinter dem Modell
  muss aber auch gespeichert werden, da er sich mit der Zeit ändert:
    * Mgl. 1: Modelle unterschiedlich benennen und mit Copy&Paste arbeiten -> wird schnell unübersichtlich
    * Mgl. 2: Wie Robert pro Modell ein eigenen Branch im Git eröffnen und immer darauf arbeiten bzw. hin und her switchen

- Konfigurierbare Modelle erstellen; Parameter nicht hard-coden!
    * Für Hyperparam-Studie ist es besser, die Werte in die Config zu packen und nur dort zu ändern
    * Es sollte möglich sein, den Kontext nur anhand der Config wiederherstellen zu können

- Set number of workers in the config.json to a higher number (e.g. 2) after finishing the debugging


NEXT STEPS:
- Logging verstehen: DONE
- Metriken erweitern
- Tensorboard verstehen und ggf erweitern
- Verwaltung, Sichern und Laden verschiedener Architekturen
- DataLoader und Dataset für neues Preprocessing anpassen: DONE



Fehlersuche:
tensoren detachen und plotten (das, was das Netz bekommt im Forward)

BiRNN + Attention rausnehmen, evtl Attention als Self-Attention


Daten nach dem CNN - noch viele 0er an den Rändern?
Evtl Probleme mit RNN, eugt nur wenn nicht richitg initialisiert


Attention nochmal anschauen: Shapes der Attentionweights

Attentionmatrix in Heatmap plotten - sollte nicht gleichverteilt sein


Cardiologist level arrythmia detection with CNNs (Andrew Ng)

Daten zwischen 0 und 1 skalieren,
Daten auf Outlier filtern, sonst wird Skalierung zerhauen
-> Robert Git Repo


Shell Skript und Anleitung wie man Netz zum Laufen bekommt
(requirements.txt)
- Daten laden (aber kein aufäwndiges Skript), am besten einfach in die Nextcloud und dann einen Link, von wo man sie kopieren kann